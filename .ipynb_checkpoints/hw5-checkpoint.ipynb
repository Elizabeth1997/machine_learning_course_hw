{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Name: Chenxin Xiong\n",
    "###  ID: 168449"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question1:  Difference between TD and MC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you imagine a scenario in which a TD update would be better on average than an Monte Carlo update? Give an example scenarioa description of past experience and a current state—in which you would expect the TD update to be better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TD method is convenient to learn one guess from the next, without waiting for an actual outcome.  \n",
    "Suppose on another day you again estimate when leaving your office that it will take 30 minutes to drive home, but then you become stuck in a massive traffic jam. 25 minutes after leaving the office you are still be stucked on the highway. However you must wait until you get home before increasing your estimate for the initial state according to the Monte Carlo approach due to the true return is unknown. However, by TD updata you can shift your initial estimate from 30 minutes toward 50 minutes immediately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question2: Off policy Q-learning and Sarsa algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use off policy Q learning to learn the optimal values of Q* (s, a). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reward on reaching on the goal (G) is 10.  \n",
    "The reward on actions that would take the agent off the grid is -1 (agent stays still in this case).  \n",
    "The reward on entering F is -5.  \n",
    "The reward on other actions is 0.  \n",
    "The discount factor γ = 0.9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rows = 4\n",
    "num_columns = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_world = np.arange(num_rows * num_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = ['up', 'down', 'left', 'right']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "opposite = {'up': 'down', 'down': 'up', 'left': 'right', 'right': 'left'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "discount_factor = 0.9\n",
    "alpha = 0.5\n",
    "e_value= 0.25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get coordinate based on the position represented by an interger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "goal_pos = grid_world[0]\n",
    "f_pos = grid_world[8]\n",
    "start_pos = grid_world[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def row_column(pos):\n",
    "    row = pos // num_columns + 1\n",
    "    column = pos % num_columns + 1\n",
    "    return row, column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def move(current_pos, direction):\n",
    "    row, column = row_column(current_pos)\n",
    "    \n",
    "    if direction == 'up':\n",
    "        new_pos = current_pos - num_columns\n",
    "    elif direction == 'down':\n",
    "        new_pos = current_pos + num_columns\n",
    "    elif direction == 'left':\n",
    "        if column == 1:\n",
    "            new_pos = -current_pos\n",
    "        else:\n",
    "            new_pos = current_pos - 1\n",
    "    elif direction == 'right':\n",
    "        if column == num_columns:\n",
    "            new_pos = -current_pos\n",
    "        else:\n",
    "            new_pos = current_pos + 1\n",
    "    else:\n",
    "        return None\n",
    "   \n",
    "    return new_pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tell if the current position is off the grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def if_off_the_grid(pos):\n",
    "    if pos not in grid_world:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_Q():\n",
    "    Q = {}\n",
    "    for pos in grid_world:\n",
    "        q = {}\n",
    "        for action in actions:\n",
    "            q[action] = 0\n",
    "        Q[pos] = q\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reward(pos):\n",
    "    if if_off_the_grid(pos):\n",
    "        return -1\n",
    "    elif pos == goal_pos:\n",
    "        # reward is 10 for goal position\n",
    "        return 10\n",
    "    elif pos == f_pos:\n",
    "        return -5\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def take_next_action(Q_value, e=None):\n",
    "    \n",
    "    li = list(Q_value.items())\n",
    "    random.shuffle(li)\n",
    "    Q_value = dict(li)\n",
    "    action = sorted(Q_value.items(), key=operator.itemgetter(1),reverse=True)[0][0]\n",
    "    \n",
    "    if e is not None:\n",
    "        if random.random() < 0.25:\n",
    "            action = random.choice(actions)\n",
    "        else:\n",
    "            pass\n",
    "    else:\n",
    "        pass\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_best_V(grid, goal, final_Q):\n",
    "    for i in grid:\n",
    "        if i != goal:\n",
    "            action = take_next_action(final_Q[i])\n",
    "            print('{}: {}'.format((i, action), final_Q[i][action]))\n",
    "        else:\n",
    "            print('{}: {}'.format((i, 'any actions'), 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_best_policy(start, goal, final_Q):\n",
    "    current = start\n",
    "    policy = {}\n",
    "    actions_list = []\n",
    "    \n",
    "    while current != goal:\n",
    "        \n",
    "        action = take_next_action(final_Q[current])\n",
    "        actions_list.append(action)\n",
    "        \n",
    "        next_position = move(current, action)\n",
    "        \n",
    "        r = get_reward(next_position)\n",
    "        policy[(current, action)] =  r\n",
    "        \n",
    "        current = next_position\n",
    "    print('Total rewards obtained: {}'.format(sum(policy.values())))\n",
    "    return policy, actions_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q learning\n",
    "<img src=\"hw5_images/Q_learning.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = initialize_Q()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    current_pos = start_pos\n",
    "    while current_pos != goal_pos:\n",
    "        action = take_next_action(Q[current_pos], e_value)\n",
    "        next_pos = move(current_pos, action)\n",
    "        reward = get_reward(next_pos)\n",
    "        if if_off_the_grid(next_pos):\n",
    "            next_pos = current_pos\n",
    "        next_action = take_next_action(Q[next_pos])\n",
    "        Q[current_pos][action] = (1 - alpha) * Q[current_pos][action] + alpha * (reward + discount_factor * Q[next_pos][next_action])\n",
    "#         print('{} -> {} -> {} -> {}'.format(current_pos, action, next_pos, next_action))\n",
    "        current_pos = next_pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Q*(s,a) for each pair of s and a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {'up': 0, 'down': 0, 'left': 0, 'right': 0},\n",
       " 1: {'up': 0, 'down': 4.481103515625, 'left': 9.990234375, 'right': 0.0},\n",
       " 2: {'up': -0.875, 'down': 0.0, 'left': 0.0, 'right': 0.0},\n",
       " 3: {'up': -0.5, 'down': -2.5, 'left': 0.0, 'right': 0.0},\n",
       " 4: {'up': -0.875, 'down': 0.0, 'left': 0.0, 'right': -0.5},\n",
       " 5: {'up': 0, 'down': 0.0, 'left': -0.5, 'right': 2.278125},\n",
       " 6: {'up': 8.940673828125, 'down': 0, 'left': 0, 'right': 0},\n",
       " 7: {'up': 0.0, 'down': 0.0, 'left': 0, 'right': 0},\n",
       " 8: {'up': 0.0, 'down': 0, 'left': 0.0, 'right': 0.0},\n",
       " 9: {'up': 0.0, 'down': 0.0, 'left': -3.75, 'right': -0.875},\n",
       " 10: {'up': 0.0, 'down': 0.0, 'left': -0.5, 'right': 0.0},\n",
       " 11: {'up': 7.6946044921875, 'down': 0, 'left': 0.0, 'right': 0.0},\n",
       " 12: {'up': 0.0,\n",
       "  'down': 0.0,\n",
       "  'left': 6.0085546875,\n",
       "  'right': 1.9862001892089847},\n",
       " 13: {'up': -3.75,\n",
       "  'down': 0.9435802423095706,\n",
       "  'left': 4.414178649902345,\n",
       "  'right': 0.16348038574218748},\n",
       " 14: {'up': 0.0, 'down': 0.0, 'left': 1.9775504333496097, 'right': -0.75},\n",
       " 15: {'up': 0.0, 'down': -0.5, 'left': -0.5, 'right': 0.0},\n",
       " 16: {'up': 0.0, 'down': 0, 'left': 0.0, 'right': 0},\n",
       " 17: {'up': 1.537734375, 'down': -0.5, 'left': 0.0, 'right': 0.0},\n",
       " 18: {'up': 2.463378387451172,\n",
       "  'down': -0.21459114013671865,\n",
       "  'left': 0.0,\n",
       "  'right': 0.0},\n",
       " 19: {'up': 0.0,\n",
       "  'down': -0.8329621865234376,\n",
       "  'left': 0.7998861730957032,\n",
       "  'right': -0.75}}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. V*(s) for each s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 'down'): 0\n",
      "(1, 'left'): 9.990234375\n",
      "(2, 'right'): 0.0\n",
      "(3, 'right'): 0.0\n",
      "(4, 'left'): 0.0\n",
      "(5, 'right'): 2.278125\n",
      "(6, 'up'): 8.940673828125\n",
      "(7, 'up'): 0.0\n",
      "(8, 'down'): 0\n",
      "(9, 'down'): 0.0\n",
      "(10, 'down'): 0.0\n",
      "(11, 'up'): 7.6946044921875\n",
      "(12, 'left'): 6.0085546875\n",
      "(13, 'left'): 4.414178649902345\n",
      "(14, 'left'): 1.9775504333496097\n",
      "(15, 'up'): 0.0\n",
      "(16, 'right'): 0\n",
      "(17, 'up'): 1.537734375\n",
      "(18, 'up'): 2.463378387451172\n",
      "(19, 'any actions'): 0\n"
     ]
    }
   ],
   "source": [
    "show_best_V(grid_world, start_pos, Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. the actions of optimal policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rewards obtained: 10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({(19, 'left'): 0,\n",
       "  (18, 'up'): 0,\n",
       "  (13, 'left'): 0,\n",
       "  (12, 'left'): 0,\n",
       "  (11, 'up'): 0,\n",
       "  (6, 'up'): 0,\n",
       "  (1, 'left'): 10},\n",
       " ['left', 'up', 'left', 'left', 'up', 'up', 'left'])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_best_policy(start_pos, 0, Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sarsa\n",
    " <img src=\"hw5_images/Sarsa.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = initialize_Q()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    current_pos = start_pos\n",
    "    action = take_next_action(Q[current_pos], e_value)\n",
    "    while current_pos != goal_pos:        \n",
    "        next_pos = move(current_pos, action)\n",
    "        reward = get_reward(next_pos)\n",
    "        if if_off_the_grid(next_pos):\n",
    "            next_pos = current_pos\n",
    "        next_action = take_next_action(Q[next_pos], e_value)\n",
    "        Q[current_pos][action] = (1 - alpha) * Q[current_pos][action] + alpha * (reward + discount_factor * Q[next_pos][next_action])\n",
    "#         print('{} -> {} -> {} -> {}'.format(current_pos, action, next_pos, next_action))\n",
    "        current_pos = next_pos\n",
    "        action = next_action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Q*(s,a) for each pair of s and a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {'up': 0, 'down': 0, 'left': 0, 'right': 0},\n",
       " 1: {'up': -0.5, 'down': 0.0, 'left': 8.75, 'right': 0.0},\n",
       " 2: {'up': -1.053125, 'down': -1.125, 'left': 0.0, 'right': 0.0},\n",
       " 3: {'up': -0.75, 'down': -2.5, 'left': 0.0, 'right': -0.225},\n",
       " 4: {'up': -0.5, 'down': -1.125, 'left': 0.0, 'right': -0.75},\n",
       " 5: {'up': 9.921875, 'down': 2.5027734375, 'left': 1.375, 'right': 1.0125},\n",
       " 6: {'up': 4.5, 'down': 0.0, 'left': -0.3375, 'right': -0.299953125},\n",
       " 7: {'up': -0.33328125, 'down': 0.0, 'left': 0.0, 'right': -3.75},\n",
       " 8: {'up': -0.225, 'down': 0.0, 'left': 0.0, 'right': 0},\n",
       " 9: {'up': 0.0, 'down': 0.0, 'left': -3.75, 'right': -0.5},\n",
       " 10: {'up': 3.2537109375,\n",
       "  'down': 0.0,\n",
       "  'left': 2.1783398437500003,\n",
       "  'right': 1.48078125},\n",
       " 11: {'up': 0.0,\n",
       "  'down': 0.0,\n",
       "  'left': 1.86328125,\n",
       "  'right': -0.06834375000000001},\n",
       " 12: {'up': -0.0759375, 'down': -0.2278125, 'left': 2.221171875, 'right': 0.0},\n",
       " 13: {'up': -4.375, 'down': 0.0, 'left': 0.0, 'right': 0.0},\n",
       " 14: {'up': 0.0, 'down': 0.0, 'left': 0.0, 'right': -0.75},\n",
       " 15: {'up': 3.22734375, 'down': -0.75, 'left': -0.5, 'right': 0.0},\n",
       " 16: {'up': 0.0, 'down': -0.5, 'left': 0.0, 'right': 0.0},\n",
       " 17: {'up': 0.307546875, 'down': -0.5, 'left': 0.0, 'right': -0.253125},\n",
       " 18: {'up': -2.25, 'down': -0.75, 'left': 0.0, 'right': -0.225},\n",
       " 19: {'up': -0.225, 'down': -0.875, 'left': -0.028125, 'right': -0.8375}}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. V*(s) for each s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 'down'): 0\n",
      "(1, 'left'): 8.75\n",
      "(2, 'right'): 0.0\n",
      "(3, 'left'): 0.0\n",
      "(4, 'left'): 0.0\n",
      "(5, 'up'): 9.921875\n",
      "(6, 'up'): 4.5\n",
      "(7, 'down'): 0.0\n",
      "(8, 'right'): 0\n",
      "(9, 'up'): 0.0\n",
      "(10, 'up'): 3.2537109375\n",
      "(11, 'left'): 1.86328125\n",
      "(12, 'left'): 2.221171875\n",
      "(13, 'right'): 0.0\n",
      "(14, 'down'): 0.0\n",
      "(15, 'up'): 3.22734375\n",
      "(16, 'up'): 0.0\n",
      "(17, 'up'): 0.307546875\n",
      "(18, 'left'): 0.0\n",
      "(19, 'any actions'): 0\n"
     ]
    }
   ],
   "source": [
    "show_best_V(grid_world, start_pos, Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. the actions of optimal policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rewards obtained: 10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({(19, 'left'): 0,\n",
       "  (18, 'left'): 0,\n",
       "  (17, 'up'): 0,\n",
       "  (12, 'left'): 0,\n",
       "  (11, 'left'): 0,\n",
       "  (10, 'up'): 0,\n",
       "  (5, 'up'): 10},\n",
       " ['left', 'left', 'up', 'left', 'left', 'up', 'up'])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_best_policy(start_pos, 0, Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Please discuss your observed difference between Sarsa algorithm and  off-policy Q-learning \n",
    "for Q-learning, the actions of optimal policy is ['left', 'up', 'left', 'left', 'up', 'up', 'left']  \n",
    "for Sarsa algorithm, it's ['left', 'left', 'up', 'left', 'left', 'up', 'up']  \n",
    "Both Q-learning and Sarsa can find best actions of optimal policy successfully. The difference between them when they play the game is Sarsa always finds a more safe path than Q-leanring (try to avoid F position). And Q-learning looks for all possible next actions a and choosing the best."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
