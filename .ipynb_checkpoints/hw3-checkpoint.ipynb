{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Name: Chenxin Xiong\n",
    "###  ID: 168449"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation of NN (using Back-Propagation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from numpy.linalg import multi_dot\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_of_samples = 2000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. generate 2000 sets of input data with 2 dimensions randomly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.rand(no_of_samples,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = x[:,0]\n",
    "x2 = x[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "metadata": {},
   "outputs": [],
   "source": [
    "ones = np.ones((1,len(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2000)"
      ]
     },
     "execution_count": 551,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ones.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_all = np.concatenate((ones, x.T), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 2000)"
      ]
     },
     "execution_count": 553,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_all.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. generate corresponding 2000 scalars as y based on a nonlinear function $f(x)=x_1+(x_2)^2 + (2^{x_1}-cos(x_2))^2 $, plus noise with Gaussian distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first para: mean; second standard deviation\n",
    "noise = np.random.normal(0, 0.2, no_of_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = x1+(x2)**2 + (2**x1-np.cos(x2))**2 + noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000,)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. split dataset into training dataset and test dataset with 8:2 ratio (80% for training and 20% for testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, Y_train, Y_test = train_test_split(x, Y, test_size=0.2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1600, 2)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1600,)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Build MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the first element indicates the input data's dimensions while the last one represents the output's\n",
    "# params: layer_info || initialization_mean || initialization_variance || bias_initialization_mean || bias_initialization_variance\n",
    "layer_info = [2, 4, 4, 1]\n",
    "no_of_weight_matrices = len(layer_info) - 1\n",
    "initialization_mean = 0\n",
    "initialization_variance = 0.1\n",
    "bias_initialization_mean = 0\n",
    "bias_initialization_variance = 0.1\n",
    "\n",
    "weight_matrices_list = []\n",
    "b_vectors_list = []\n",
    "w_size_list = []\n",
    "b_size_list = []\n",
    "\n",
    "for i in range(no_of_weight_matrices):\n",
    "    # plus one is for adding one additional dimension for bias term\n",
    "    w_size = (layer_info[no_of_weight_matrices-i], layer_info[no_of_weight_matrices-i-1])\n",
    "    b_size = (layer_info[no_of_weight_matrices-i], 1)\n",
    "    # initialize each layer's weight using Gaussian distribution\n",
    "    w = np.random.normal(initialization_mean, initialization_variance, w_size)\n",
    "    b = np.random.normal(bias_initialization_mean, bias_initialization_variance, b_size)\n",
    "    \n",
    "    w_size_list.insert(0, w_size)\n",
    "    weight_matrices_list.insert(0, w)\n",
    "    b_size_list.insert(0, b_size)\n",
    "    b_vectors_list.insert(0, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-0.19859593, -0.00641315],\n",
       "        [-0.00026687, -0.06757579],\n",
       "        [-0.18066387, -0.10159967],\n",
       "        [ 0.07284424,  0.25489208]]),\n",
       " array([[-0.04740351,  0.1811947 ,  0.08491296,  0.05851664],\n",
       "        [-0.06267253, -0.0424983 ,  0.21179961, -0.06175603],\n",
       "        [ 0.29986473, -0.12863928, -0.03229859,  0.00192423],\n",
       "        [-0.00710049,  0.09213986,  0.09068238, -0.10100174]]),\n",
       " array([[ 0.03777835,  0.00497708, -0.04046985, -0.01809298]])]"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight_matrices_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(4, 2), (4, 4), (1, 4)]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_size_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-0.02404253],\n",
       "        [-0.03045002],\n",
       "        [ 0.08509289],\n",
       "        [-0.12219039]]), array([[ 0.00458364],\n",
       "        [-0.22790025],\n",
       "        [-0.00907324],\n",
       "        [ 0.05083756]]), array([[0.08996917]])]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_vectors_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(4, 1), (4, 1), (1, 1)]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_size_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(x, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_derivative(x):\n",
    "    x[x<=0] = 0\n",
    "    x[x>0] = 1\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RMSE(output, target):\n",
    "    output = output.squeeze()\n",
    "    rmse = np.sqrt(np.mean((output-target)**2))\n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "# direct output result of each layer\n",
    "z_list = []\n",
    "# relu(direct output result) of each layer\n",
    "a = x_train.T\n",
    "a_list = [a]\n",
    "for i in range(len(weight_matrices_list)-1):\n",
    "    # wx+b\n",
    "    z = np.matmul(weight_matrices_list[i], a) + b_vectors_list[i]\n",
    "    z_list.append(z)\n",
    "    a = relu(z)\n",
    "    # add bias term\n",
    "#     a = np.concatenate((np.ones((1,a.shape[-1])), a), axis=0)\n",
    "    a_list.append(a)\n",
    "# output layer\n",
    "output = np.matmul(weight_matrices_list[-1], a_list[-1])\n",
    "rmse = RMSE(output, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 1600)"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.5490337164666608"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### backward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_matrices_gradient_list = []\n",
    "b_vectors_gradient_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{\\partial c}{\\partial y}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the gradient of each layer's weight matrix\n",
    "c_y = (1/len(Y_train)) * (output-Y_train.reshape(1, len(Y_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1600)"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### calculate the gradient of the weight matrix connected with the output layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{\\partial c}{\\partial w_n}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_layer_gradient = np.matmul(c_y, a_list[-1].T)\n",
    "weight_matrices_gradient_list.append(output_layer_gradient)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{\\partial c}{\\partial b_n}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_layer_gradient_b = np.matmul(c_y, np.ones((c_y.shape[1],1)))\n",
    "b_vectors_gradient_list.append(output_layer_gradient_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### calculate the gradient of the weight matrices connected with the input layer and hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_weight_matrices_list = weight_matrices_list[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 0.03777835,  0.00497708, -0.04046985, -0.01809298]]),\n",
       " array([[-0.04740351,  0.1811947 ,  0.08491296,  0.05851664],\n",
       "        [-0.06267253, -0.0424983 ,  0.21179961, -0.06175603],\n",
       "        [ 0.29986473, -0.12863928, -0.03229859,  0.00192423],\n",
       "        [-0.00710049,  0.09213986,  0.09068238, -0.10100174]]),\n",
       " array([[-0.19859593, -0.00641315],\n",
       "        [-0.00026687, -0.06757579],\n",
       "        [-0.18066387, -0.10159967],\n",
       "        [ 0.07284424,  0.25489208]])]"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_weight_matrices_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_a = c_y\n",
    "# doesn't count the last weight matrices\n",
    "for i in range(no_of_weight_matrices-1):\n",
    "#     weights = [w.T for w in r_weight_matrices_list[0:i+1]]\n",
    "#     weights.append(c_y)\n",
    "#     c_a = multi_dot(weights)\n",
    "    weight = r_weight_matrices_list[i]\n",
    "    c_a = np.matmul(weight.T, c_a)\n",
    "    a_z = relu_derivative(z_list[-1-i])\n",
    "    c_z = c_a * a_z\n",
    "    z_b = np.ones((c_z.shape[1],1))\n",
    "    z_w = a_list[-2-i]\n",
    "    c_b = np.matmul(c_z, z_b)\n",
    "    c_w = np.matmul(c_z, z_w.T)\n",
    "    b_vectors_gradient_list.insert(0, c_b)\n",
    "    weight_matrices_gradient_list.insert(0, c_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ],\n",
       "        [-0.00274866, -0.00282885]]),\n",
       " array([[ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        , -0.00069481],\n",
       "        [ 0.        ,  0.        ,  0.        ,  0.00564966],\n",
       "        [ 0.        ,  0.        ,  0.        ,  0.        ]]),\n",
       " array([[ 0.        ,  0.        ,  0.        , -0.00274866],\n",
       "        [ 0.        ,  0.        ,  0.        , -0.00282885]]),\n",
       " array([[ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "        [ 0.        , -0.00069481,  0.00564966,  0.        ]]),\n",
       " array([[ 0.        ,  0.        ,  0.        , -0.00274866],\n",
       "        [ 0.        ,  0.        ,  0.        , -0.00282885]]),\n",
       " array([[ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "        [ 0.        , -0.00069481,  0.00564966,  0.        ]]),\n",
       " array([[ 0.        , -0.01725745, -0.19262706,  0.        ]])]"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight_matrices_gradient_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 0.       ],\n",
       "        [ 0.       ],\n",
       "        [ 0.       ],\n",
       "        [-0.0042655]]), array([[ 0.        ],\n",
       "        [-0.00648644],\n",
       "        [ 0.05274282],\n",
       "        [ 0.        ]]), array([[ 0.       ],\n",
       "        [ 0.       ],\n",
       "        [ 0.       ],\n",
       "        [-0.0042655]]), array([[ 0.        ],\n",
       "        [-0.00648644],\n",
       "        [ 0.05274282],\n",
       "        [ 0.        ]]), array([[ 0.       ],\n",
       "        [ 0.       ],\n",
       "        [ 0.       ],\n",
       "        [-0.0042655]]), array([[ 0.        ],\n",
       "        [-0.00648644],\n",
       "        [ 0.05274282],\n",
       "        [ 0.        ]]), array([[-1.30326194]])]"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_vectors_gradient_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. update weights and bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_matrices_list = [weight_matrices_list[i] - lr * weight_matrices_gradient_list[i] for i in range(no_of_weight_matrices)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_vectors_list = [b_vectors_list[i] - lr * b_vectors_gradient_list[i] for i in range(no_of_weight_matrices)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. combine all implemented parts above together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### relevant parameters set up\n",
    "1. the number of inputs: 2\n",
    "2. the number of outputs: 1\n",
    "3. the number of hidden neurons: 12 (3 hidden layers, each layer has 4 neurons)\n",
    "4. the number of weights: (2x4+4)+(4x4+4)+(4x4+4)+(4x1+1)=57"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loss_dict = {}\n",
    "learning_rate_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loss_list_whole = []\n",
    "test_loss_list_whole = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loss_list_whole_4 = []\n",
    "test_loss_list_whole_4 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_info = [2, 4, 4, 4, 1]\n",
    "no_of_weight_matrices = len(layer_info) - 1\n",
    "\n",
    "initialization_mean = 0.2\n",
    "initialization_standard_deviation = 0.01\n",
    "bias_initialization_mean = 0.2\n",
    "bias_initialization_standard_deviation = 0.01\n",
    "\n",
    "weight_matrices_list = []\n",
    "b_vectors_list = []\n",
    "w_size_list = []\n",
    "b_size_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "no_of_iterations = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(no_of_weight_matrices):\n",
    "    # plus one is for adding one additional dimension for bias term\n",
    "    w_size = (layer_info[no_of_weight_matrices-i], layer_info[no_of_weight_matrices-i-1])\n",
    "    b_size = (layer_info[no_of_weight_matrices-i], 1)\n",
    "    # initialize each layer's weight using Gaussian distribution\n",
    "    w = np.random.normal(initialization_mean, initialization_standard_deviation, w_size)\n",
    "    b = np.random.normal(bias_initialization_mean, bias_initialization_standard_deviation, b_size)\n",
    "    \n",
    "    w_size_list.insert(0, w_size)\n",
    "    weight_matrices_list.insert(0, w)\n",
    "    b_size_list.insert(0, b_size)\n",
    "    b_vectors_list.insert(0, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loss_list = []\n",
    "test_loss_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(a, weight_matrices_list, b_vectors_list):\n",
    "    for i in range(len(weight_matrices_list)-1):\n",
    "        z = np.matmul(weight_matrices_list[i], a) + b_vectors_list[i]\n",
    "        a = relu(z)\n",
    "    output = np.matmul(weight_matrices_list[-1], a)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_weight_matrices = None\n",
    "v_b_vectors = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_matrices_gradient_square = None\n",
    "b_vectors_matrices_gradient_square = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, training RMSE 1.1231, test RMSE 1.128177\n",
      "Epoch 100, training RMSE 1.0024, test RMSE 1.004358\n",
      "Epoch 200, training RMSE 0.9459, test RMSE 0.946097\n",
      "Epoch 300, training RMSE 0.9031, test RMSE 0.901785\n",
      "Epoch 400, training RMSE 0.8684, test RMSE 0.865689\n",
      "Epoch 500, training RMSE 0.8396, test RMSE 0.835530\n",
      "Epoch 600, training RMSE 0.8154, test RMSE 0.810081\n",
      "Epoch 700, training RMSE 0.7950, test RMSE 0.788542\n",
      "Epoch 800, training RMSE 0.7778, test RMSE 0.770325\n",
      "Epoch 900, training RMSE 0.7634, test RMSE 0.754958\n",
      "Epoch 1000, training RMSE 0.7514, test RMSE 0.742041\n",
      "Epoch 1100, training RMSE 0.7414, test RMSE 0.731222\n",
      "Epoch 1200, training RMSE 0.7331, test RMSE 0.722191\n",
      "Epoch 1300, training RMSE 0.7262, test RMSE 0.714671\n",
      "Epoch 1400, training RMSE 0.7206, test RMSE 0.708418\n",
      "Epoch 1500, training RMSE 0.7159, test RMSE 0.703220\n",
      "Epoch 1600, training RMSE 0.7120, test RMSE 0.698890\n",
      "Epoch 1700, training RMSE 0.7088, test RMSE 0.695272\n",
      "Epoch 1800, training RMSE 0.7061, test RMSE 0.692231\n",
      "Epoch 1900, training RMSE 0.7038, test RMSE 0.689655\n",
      "Epoch 2000, training RMSE 0.7019, test RMSE 0.687452\n",
      "Epoch 2100, training RMSE 0.7002, test RMSE 0.685547\n",
      "Epoch 2200, training RMSE 0.6987, test RMSE 0.683878\n",
      "Epoch 2300, training RMSE 0.6974, test RMSE 0.682396\n",
      "Epoch 2400, training RMSE 0.6962, test RMSE 0.681060\n",
      "Epoch 2500, training RMSE 0.6951, test RMSE 0.679841\n",
      "Epoch 2600, training RMSE 0.6940, test RMSE 0.678712\n",
      "Epoch 2700, training RMSE 0.6931, test RMSE 0.677655\n",
      "Epoch 2800, training RMSE 0.6921, test RMSE 0.676654\n",
      "Epoch 2900, training RMSE 0.6912, test RMSE 0.675697\n",
      "Epoch 3000, training RMSE 0.6903, test RMSE 0.674775\n",
      "Epoch 3100, training RMSE 0.6895, test RMSE 0.673880\n",
      "Epoch 3200, training RMSE 0.6886, test RMSE 0.673006\n",
      "Epoch 3300, training RMSE 0.6878, test RMSE 0.672150\n",
      "Epoch 3400, training RMSE 0.6870, test RMSE 0.671307\n",
      "Epoch 3500, training RMSE 0.6861, test RMSE 0.670476\n",
      "Epoch 3600, training RMSE 0.6853, test RMSE 0.669652\n",
      "Epoch 3700, training RMSE 0.6845, test RMSE 0.668836\n",
      "Epoch 3800, training RMSE 0.6837, test RMSE 0.668025\n",
      "Epoch 3900, training RMSE 0.6829, test RMSE 0.667219\n",
      "Epoch 4000, training RMSE 0.6820, test RMSE 0.666416\n",
      "Epoch 4100, training RMSE 0.6812, test RMSE 0.665617\n",
      "Epoch 4200, training RMSE 0.6804, test RMSE 0.664819\n",
      "Epoch 4300, training RMSE 0.6796, test RMSE 0.664024\n",
      "Epoch 4400, training RMSE 0.6788, test RMSE 0.663229\n",
      "Epoch 4500, training RMSE 0.6780, test RMSE 0.662436\n",
      "Epoch 4600, training RMSE 0.6772, test RMSE 0.661644\n",
      "Epoch 4700, training RMSE 0.6764, test RMSE 0.660853\n",
      "Epoch 4800, training RMSE 0.6756, test RMSE 0.660062\n",
      "Epoch 4900, training RMSE 0.6747, test RMSE 0.659272\n"
     ]
    }
   ],
   "source": [
    "for iteration in range(no_of_iterations):\n",
    "    z_list = []\n",
    "    a = x_train.T\n",
    "    a_list = [a]\n",
    "    \n",
    "    for i in range(len(weight_matrices_list)-1):\n",
    "        # wx+b\n",
    "        z = np.matmul(weight_matrices_list[i], a) + b_vectors_list[i]\n",
    "        z_list.append(z)\n",
    "        a = relu(z)\n",
    "        a_list.append(a)\n",
    "    # train\n",
    "    output = np.matmul(weight_matrices_list[-1], a_list[-1])\n",
    "    rmse = RMSE(output, Y_train)\n",
    "    # test\n",
    "    test_output = test(x_test.T, weight_matrices_list, b_vectors_list)\n",
    "    test_rmse = RMSE(test_output, Y_test)\n",
    "    \n",
    "    training_loss_list.append(rmse)\n",
    "    test_loss_list.append(test_rmse)\n",
    "    \n",
    "    if iteration % 100 == 0:\n",
    "        print('Epoch {}, training RMSE {:.4f}, test RMSE {:4f}'.format(iteration, rmse, test_rmse))\n",
    "    \n",
    "    weight_matrices_gradient_list = []\n",
    "    b_vectors_gradient_list = []\n",
    "    c_y = (1/len(Y_train)) * (output-Y_train.reshape(1, len(Y_train)))\n",
    "    output_layer_gradient = np.matmul(c_y, a_list[-1].T)\n",
    "    weight_matrices_gradient_list.append(output_layer_gradient)\n",
    "    output_layer_gradient_b = np.matmul(c_y, np.ones((c_y.shape[1],1)))\n",
    "    b_vectors_gradient_list.append(output_layer_gradient_b)\n",
    "    r_weight_matrices_list = weight_matrices_list[::-1]\n",
    "    c_a = c_y\n",
    "    \n",
    "    for i in range(no_of_weight_matrices-1):\n",
    "        weight = r_weight_matrices_list[i]\n",
    "        c_a = np.matmul(weight.T, c_a)\n",
    "        a_z = relu_derivative(z_list[-1-i])\n",
    "        c_z = c_a * a_z\n",
    "        z_b = np.ones((c_z.shape[1],1))\n",
    "        z_w = a_list[-2-i]\n",
    "        c_b = np.matmul(c_z, z_b)\n",
    "        c_w = np.matmul(c_z, z_w.T)\n",
    "        b_vectors_gradient_list.insert(0, c_b)\n",
    "        weight_matrices_gradient_list.insert(0, c_w)\n",
    "        \n",
    "#     v_weight_matrices = momentum(v_weight_matrices, weight_matrices_gradient_list)\n",
    "#     v_b_vectors = momentum(v_b_vectors, b_vectors_gradient_list)\n",
    "    \n",
    "#     weight_matrices_list = [weight_matrices_list[i] - lr * v_weight_matrices[i] for i in range(no_of_weight_matrices)]\n",
    "#     b_vectors_list = [b_vectors_list[i] - lr * v_b_vectors[i] for i in range(no_of_weight_matrices)]\n",
    "    weight_matrices_gradient_square = adaGrad(weight_matrices_gradient_square, weight_matrices_gradient_list)\n",
    "    b_vectors_matrices_gradient_square = adaGrad(b_vectors_matrices_gradient_square, b_vectors_gradient_list)\n",
    "    \n",
    "    weight_matrices_list = [weight_matrices_list[i] - lr * (weight_matrices_gradient_list[i]/(np.sqrt(weight_matrices_gradient_square[i])+ 1e-7)) for i in range(no_of_weight_matrices)]\n",
    "    b_vectors_list = [b_vectors_list[i] - lr * (b_vectors_gradient_list[i]/(np.sqrt(b_vectors_matrices_gradient_square[i])+ 1e-7)) for i in range(no_of_weight_matrices)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loss_dict[learning_rate] = training_loss_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) Plot a figure of how the error decreases during learning (lr=0.01 is chosen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "# fig.add_trace(go.Scatter(y=training_loss_list,\n",
    "#                         mode='lines',\n",
    "#                         name=\"Learning Rate {}\".format(learning_rate)))\n",
    "for k, v in training_loss_dict.items():\n",
    "    fig.add_trace(go.Scatter(y=v,\n",
    "                        mode='lines',\n",
    "                        name=\"Learning Rate {}\".format(k)))\n",
    "fig.update_layout(\n",
    "    title='',\n",
    "    xaxis_title=\"Times of Iterations\",\n",
    "    yaxis_title=\"Training Error (RMSE)\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"hw3_images/1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) Show the root mean squared error (RMSE) when applying the iteratively trained NN on the training set (in one color), and on the testing test (in the other color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(y=training_loss_list,\n",
    "                        mode='lines',\n",
    "                        name=\"training\"))\n",
    "fig.add_trace(go.Scatter(y=test_loss_list,\n",
    "                        mode='lines',\n",
    "                        name=\"test\"))\n",
    "fig.update_layout(\n",
    "    title='',\n",
    "    xaxis_title=\"Times of Iterations\",\n",
    "    yaxis_title=\"RMSE\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"hw3_images/2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) compare the training error  and testing error decreasing curve in 3 independent runs with different initialized weights. \n",
    "1. Gaussian distribution initialization (mean=0, standard deviation=0.1)\n",
    "2. Gaussian distribution initialization (mean=0.2, standard deviation=0.01)\n",
    "3. Gaussian distribution initialization (mean=0.5, standard deviation=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loss_list_whole.append(training_loss_list)\n",
    "test_loss_list_whole.append(test_loss_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_list = ['#EF553B','blue']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = make_subplots(\n",
    "    rows=1, cols=3,\n",
    "    subplot_titles=(\"Gaussian(0/0.1)\", \"Gaussian(0.2/0.01)\", \"Gaussian(0.5/0.001)\"))\n",
    "\n",
    "fig.add_trace(go.Scatter(y=training_loss_list_whole[0], legendgroup=\"group\", name=\"training\", mode='lines', line=dict(color=color_list[0]), showlegend=True),\n",
    "              row=1, col=1)\n",
    "fig.add_trace(go.Scatter(y=test_loss_list_whole[0], name=\"test\", mode='lines', line=dict(color=color_list[1])),\n",
    "              row=1, col=1)\n",
    "\n",
    "fig.add_trace(go.Scatter(y=training_loss_list_whole[1], legendgroup=\"group\", name=\"training\", mode='lines', line=dict(color=color_list[0])),\n",
    "              row=1, col=2)\n",
    "fig.add_trace(go.Scatter(y=test_loss_list_whole[1], name=\"test\", mode='lines', line=dict(color=color_list[1])),\n",
    "              row=1, col=2)\n",
    "\n",
    "fig.add_trace(go.Scatter(y=training_loss_list_whole[2], legendgroup=\"group\", name=\"training\", mode='lines', line=dict(color=color_list[0])),\n",
    "              row=1, col=3)\n",
    "fig.add_trace(go.Scatter(y=test_loss_list_whole[2], name=\"test\", mode='lines', line=dict(color=color_list[1])),\n",
    "              row=1, col=3)\n",
    "\n",
    "fig.update_layout(height=500, width=1100,\n",
    "                  title_text=\"Three different initialized weights (orange: training, blue: test)\")\n",
    "\n",
    "fig.update_layout(showlegend=True)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"hw3_images/3.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4) show the training error curve and the testing error curve when NN has M/2 hidden units, or has 2*M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loss_list_whole_4.append(training_loss_list)\n",
    "test_loss_list_whole_4.append(test_loss_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = make_subplots(\n",
    "    rows=1, cols=2,\n",
    "    subplot_titles=(\"M/2 hidden neurons (2)\", \"M*2 hidden neurons (8)\"))\n",
    "\n",
    "fig.add_trace(go.Scatter(y=training_loss_list_whole_4[0], name=\"training\", mode='lines', line=dict(color=color_list[0])),\n",
    "              row=1, col=1)\n",
    "fig.add_trace(go.Scatter(y=test_loss_list_whole_4[0], name=\"test\", mode='lines', line=dict(color=color_list[1])),\n",
    "              row=1, col=1)\n",
    "\n",
    "fig.add_trace(go.Scatter(y=training_loss_list_whole_4[1],name=\"training\", mode='lines', line=dict(color=color_list[0])),\n",
    "              row=1, col=2)\n",
    "fig.add_trace(go.Scatter(y=test_loss_list_whole_4[1], name=\"test\", mode='lines', line=dict(color=color_list[1])),\n",
    "              row=1, col=2)\n",
    "\n",
    "\n",
    "fig.update_layout(height=500, width=1100,\n",
    "                  title_text=\"Two different settings of the number of hidden neurons (orange: training, blue: test)\")\n",
    "\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"hw3_images/4.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus:  Implement two types of gradient descent optimization strategies discussed in class (e.g., choosing two from momentum, AdaGrad, RMSProp, AdaDelta or Adam)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [],
   "source": [
    "def momentum(ipt, ipt_gradient, rho=0.9):\n",
    "    if ipt is None:\n",
    "        return ipt_gradient\n",
    "    else:\n",
    "        return [rho * ipt[i] + ipt_gradient[i] for i in range(len(ipt_gradient))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(y=training_loss_list,\n",
    "                        mode='lines',\n",
    "                        name=\"training\"))\n",
    "fig.add_trace(go.Scatter(y=test_loss_list,\n",
    "                        mode='lines',\n",
    "                        name=\"test\"))\n",
    "fig.update_layout(\n",
    "    title='Momentum (rho=0.9)',\n",
    "    xaxis_title=\"Times of Iterations\",\n",
    "    yaxis_title=\"RMSE\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"hw3_images/5.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### same learning rate and the number of hidden neurons, using momentum converges much faster (after 1500 iterations) and obtains much smaller training and test RMSE (0.259916) cause it has the momentum to get out of the local minimals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adaGrad(gradient_square_list, gradient_list):\n",
    "    if gradient_square_list is None:\n",
    "        return [gradient * gradient for gradient in gradient_list]\n",
    "    else:\n",
    "        return [gradient_square_list[i] + gradient * gradient for i, gradient in enumerate(gradient_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(y=training_loss_list,\n",
    "                        mode='lines',\n",
    "                        name=\"training\"))\n",
    "fig.add_trace(go.Scatter(y=test_loss_list,\n",
    "                        mode='lines',\n",
    "                        name=\"test\"))\n",
    "fig.update_layout(\n",
    "    title='AdaGrad',\n",
    "    xaxis_title=\"Times of Iterations\",\n",
    "    yaxis_title=\"RMSE\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"hw3_images/6.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The loss is smaller while it doesn't converge faster. The loss curve is much more smooth. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
